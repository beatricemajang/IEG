{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alzheimer's Disease üß†üîçüìà\n",
    "Prepared by Syahmi, Beatrice & Farriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects older adults, leading to the gradual loss of memory, cognitive function, and the ability to perform everyday activities. It is the most common cause of dementia, accounting for 60-80% of all dementia cases.\n",
    "\n",
    "The exact cause of Alzheimer's disease is not fully understood, but it is believed to result from a combination of genetic, environmental, and lifestyle factors. Some key risk factors include: age, family history, genetics, lifestyle and heart health, and head injury.\n",
    "\n",
    "We are representative from Alzheimer's organization.\n",
    "\n",
    "Roles:\n",
    "\n",
    "Project Manager (Mohamad Farriz Fikri)\n",
    "Data Analyst (Beatrice Majang)\n",
    "Machine Learning Engineer (Syahmi Sade)\n",
    "Problem Statement:\n",
    "\n",
    "There is a pressing need to analyze the available data to gain insights into the factors contributing to the disease's onset and progression, which can help in early detection and improved management of the condition\n",
    "The data we used is from a Hospital in the United States. We will not reveal the hospital as for confidential purpose.\n",
    "\n",
    "Objective:\n",
    "\n",
    "Data visualization\n",
    "Machine Learning: Compare models of Decision Tree, Random Forest, KNN, and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section involves data analysis and machine learning model implementation using various Python libraries. It includes importing necessary libraries for data manipulation, visualization, preprocessing, and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sns.set_theme(context='notebook', palette='deep', style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Converting the data set into data frame structure\n",
    "df = pd.read_csv('/content/alzheimers_disease_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process involves a thorough examination of the dataset to understand its structure, data types, and overall content. We will display the initial overview of the data. Additionally, we'll review the dataset's information summary and data types for each column. Summary statistics for numerical columns will be generated to understand their distributions. We'll also check the number of unique values in categorical columns and identify any duplicate rows.\n",
    "\n",
    "In this notebook, we performed a comprehensive data inspection on an Alzheimer's disease dataset. We explored the dataset structure, reviewed its information summary, and generated statistical summaries. This inspection provides a better understanding of the dataset and prepares it for further analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Displaying the data frame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Displaying the info for data columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to format the values become readable\n",
    "def format_value(x):\n",
    "    if abs(x) < 1e-2 and x != 0:\n",
    "        return f'{x:.2e}'\n",
    "    else:\n",
    "        return f'{x:.2f}'\n",
    "\n",
    "# Displaying the describe of the data frame\n",
    "read = df.describe().T\n",
    "readsummary = read.applymap(format_value)\n",
    "readsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# To check if there is any duplicated values inside the data set\n",
    "sum(df.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# To confirm the counts of the DoctorInCharge\n",
    "df.DoctorInCharge.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing is a critical step in preparing a dataset for analysis and modeling. For an Alzheimer's disease dataset, this process typically involves several key steps to clean and structure the data effectively. This includes removing unnecessary columns such as DoctorInCharge and PatientID, as well as identifying the unique values within specific DataFrame columns to understand the data distribution and ensure consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['PatientID', 'DoctorInCharge'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe unique values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#unique values in each column\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in column '{column}':\")\n",
    "    print(unique_values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization helps in identifying patterns, trends, and relationships between variables. For an Alzheimer's disease dataset, various types of visualizations can be employed to gain insights into clinical, genetic, and demographic data. In this case, we use bar charts to visualize the frequency distribution of categorical variables such as gender, ethnicity, and education level etc.\n",
    "\n",
    "The dataset comprises 2,149 observations, with all values being non-null and numerical. There are no duplicate records present. After excluding the DoctorInCharge and PatientID columns, the dataset now includes 33 features. By employing visualization technique, we can gain insights into the distribution of features. This helps in making informed decisions for further analysis, feature engineering, and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Identify numerical columns: columns with more than 10 unique values are considered numerical\n",
    "numerical_columns = [col for col in df.columns if df[col].nunique() > 10]\n",
    "\n",
    "# Identify categorical columns: columns that are not numerical and not 'Diagnosis'\n",
    "categorical_columns = df.columns.difference(numerical_columns).difference(['Diagnosis']).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Custom labels for the categorical columns\n",
    "custom_labels = {\n",
    "    'Gender': ['Male', 'Female'],\n",
    "    'Ethnicity': ['Caucassian', 'African American', 'Asian', 'Others'],\n",
    "    'EducationLevel': ['None', 'High School', 'Bachelor\\'s', 'Higher'],\n",
    "    'Smoking': ['No', 'Yes'],\n",
    "    'FamilyHistoryAlzheimers': ['No', 'Yes'],\n",
    "    'CardiovascularDisease': ['No', 'Yes'],\n",
    "    'Diabetes': ['No', 'Yes'],\n",
    "    'Depression': ['No', 'Yes'],\n",
    "    'HeadInjury': ['No', 'Yes'],\n",
    "    'Hypertension': ['No', 'Yes'],\n",
    "    'MemoryComplaints': ['No', 'Yes'],\n",
    "    'BehavioralProblems': ['No', 'Yes'],\n",
    "    'Confusion': ['No', 'Yes'],\n",
    "    'Disorientation': ['No', 'Yes'],\n",
    "    'PersonalityChanges': ['No', 'Yes'],\n",
    "    'DifficultyCompletingTasks': ['No', 'Yes'],\n",
    "    'Forgetfulness': ['No', 'Yes']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "# Number of columns to plot side by side\n",
    "n_cols = 2\n",
    "n_rows = (len(categorical_columns) + n_cols - 1) // n_cols  # Calculate the number of rows needed\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 5))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(categorical_columns):\n",
    "    sns.countplot(data=df, x=column, palette=custom_palette, ax=axes[i])\n",
    "    axes[i].set_title(f'Countplot of {column}', fontweight='bold')\n",
    "\n",
    "    # Set custom labels\n",
    "    labels = custom_labels[column]\n",
    "    ticks = range(len(labels))\n",
    "    axes[i].set_xticks(ticks)\n",
    "    axes[i].set_xticklabels(labels, fontstyle='italic', fontweight='bold')\n",
    "\n",
    "    # Set axis labels to italic\n",
    "    axes[i].set_xlabel(axes[i].get_xlabel(), fontstyle='italic')\n",
    "    axes[i].set_ylabel(axes[i].get_ylabel(), fontstyle='italic')\n",
    "\n",
    "# Remove empty subplots if the number of categorical columns is not even\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust space between plots\n",
    "plt.subplots_adjust(wspace=20, hspace=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n_rows = -(-len(numerical_columns) // n_cols)  # Ceiling division to get the number of rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 30))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each numerical column\n",
    "for idx, column in enumerate(numerical_columns):\n",
    "    sns.histplot(data=df, x=column, kde=True, bins=20, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution of {column}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Remove any empty subplots\n",
    "for i in range(len(numerical_columns), len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Heatgraph\n",
    "2. Pearson correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(),cmap=\"coolwarm\", cbar_kws={\"shrink\": .5}, mask=mask)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compute Pearson correlation coefficients\n",
    "correlations = df.corr(numeric_only=True)['Diagnosis'][:-1].sort_values()\n",
    "\n",
    "# Set the size of the figure\n",
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "# Create a bar plot of the Pearson correlation coefficients\n",
    "ax = correlations.plot(kind='bar', width=0.7)\n",
    "\n",
    "# Set the y-axis limits and labels\n",
    "ax.set(ylim=[-1, 1], ylabel='Pearson Correlation', xlabel='Features',\n",
    "       title='Pearson Correlation with Diagnosis')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "ax.set_xticklabels(correlations.index, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs for top 5 highlights features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Functional Assessment Scores by Diagnosis\n",
    "2. Activities of Daily Living Score by Diagnosis\n",
    "3. Mini-mental state examination score by Diagnosis\n",
    "4. Behavioral Problems by Diagnosis\n",
    "5. Distribution of Memories by Diagnosis\n",
    "We also include what is the amount of patients with Alzheimer's Disease based on the total patients from the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sns.swarmplot(data=df, y='FunctionalAssessment', x='Diagnosis')\n",
    "plt.title(f'Distribution of Functional Assessment Scores by Diagnosis Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training where we use historical data to teach the model how to make predictions. In this case, we use the model of decision tree, random forest, K-Nearest neighbors, and logistic regression. Some parameters have been tested and the dataset have been split into training and testing set to evaluate the model's performance on unseen data.\n",
    "\n",
    "Proper evaluation ensures that the model is reliable, generalizes well to new data, and provides actionable insights. For Alzheimer's disease prediction, we use common metrics such as F1 score. The F1 score is a metric used to evaluate the performance of a classification model, especially when dealing with imbalanced datasets. It combines both precision and recall into a single metric by calculating their harmonic mean. This provides a balance between the two, making it useful for scenarios where both false positives and false negatives are important. While a confusion matrix provides a detailed breakdown of prediction performance by showing the counts of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Main goal: To choose which models have the highest F1 Score to detect if a patients is an Alzheimer's Disease patients or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize & Standardize the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['Age', 'BMI', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality', 'SleepQuality', 'SystolicBP', 'DiastolicBP',\n",
    "           'CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL', 'CholesterolTriglycerides', 'MMSE', 'FunctionalAssessment', 'ADL']\n",
    "\n",
    "#normalize the columns\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[columns] = min_max_scaler.fit_transform(df[columns])\n",
    "\n",
    "#standardize the columns\n",
    "standard_scaler = StandardScaler()\n",
    "df[columns] = standard_scaler.fit_transform(df[columns])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix for the models & The difference of F1 Score between the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming df is already defined and contains the necessary data\n",
    "# Split data into features and target\n",
    "X = df.drop(columns=['Diagnosis'])\n",
    "y = df['Diagnosis']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'Decision Tree': {'max_depth': [3, 5, 7, 12, None]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 12, None]},\n",
    "    'K-Nearest Neighbors': {'n_neighbors': [3, 5, 7]},\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]}\n",
    "}\n",
    "\n",
    "# Instantiate classification models with default parameters\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "\n",
    "# Dictionary to store the F1 scores for each model\n",
    "f1_scores = {\n",
    "    'Model': [],\n",
    "    'Dataset': [],\n",
    "    'F1 Score': []\n",
    "}\n",
    "\n",
    "# Fit models using GridSearchCV for hyperparameter tuning\n",
    "num_models = len(models)\n",
    "fig, axes = plt.subplots((num_models + 1) // 2, 2, figsize=(15, (num_models // 2 + num_models % 2) * 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict on test set\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    f1_test = f1_score(y_test, y_test_pred)\n",
    "    f1_scores['Model'].append(name)\n",
    "    f1_scores['Dataset'].append('Test')\n",
    "    f1_scores['F1 Score'].append(f1_test)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name} Confusion Matrix', fontweight='bold')\n",
    "\n",
    "    # Print F1 score for test set\n",
    "    # print(name)\n",
    "    # print(f'F1 Score for Test: {f1_test:.2f}\\n')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convert the F1 scores to a DataFrame for easy plotting\n",
    "f1_scores_df = pd.DataFrame(f1_scores)\n",
    "\n",
    "# Filter out training dataset scores for the bar chart\n",
    "f1_scores_test_df = f1_scores_df[f1_scores_df['Dataset'] == 'Test']\n",
    "\n",
    "# Plot the F1 scores using a bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x='Model', y='F1 Score', data=f1_scores_test_df, palette='Set2')\n",
    "\n",
    "# Add F1 scores on top of the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.2f}',\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center',\n",
    "                xytext=(0, 10),\n",
    "                textcoords='offset points',\n",
    "                fontweight='bold')\n",
    "\n",
    "plt.title('F1 Scores for Different Models (Test Dataset)', fontweight='bold')\n",
    "plt.xlabel('Model', fontweight='bold')\n",
    "plt.ylabel('F1 Score', fontstyle='italic')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "1. Exploratory Data Analysis (EDA) for Alzheimer's disease data is a critical first step in understanding the disease's impact and its underlying factors. We can uncover significant patterns and trends that inform our understanding of Alzheimer's disease in United States.\n",
    "\n",
    "Recommendations\n",
    "1. Enhanced Data Collection: Improve the collection and availability of high-quality, comprehensive data on Alzheimer's disease in United States, including demographic, clinical, and socio-economic information.\n",
    "\n",
    "2. Public Health Strategies: Use insights from EDA to inform targeted public health strategies and interventions aimed at early detection, prevention, and management of Alzheimer's disease.\n",
    "\n",
    "3. Focused Research: Encourage further research based on the identified trends and significant variables from the EDA to develop predictive models and inferential statistics that can provide deeper insights into the disease.\n",
    "\n",
    "4. Awareness and Education: Increase awareness and education about Alzheimer's disease among healthcare professionals and the general public to promote early diagnosis and effective management.\n",
    "\n",
    "5. Policy Development: Support the development of policies that address the needs of Alzheimer's patients and their caregivers, ensuring access to necessary resources and support systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
